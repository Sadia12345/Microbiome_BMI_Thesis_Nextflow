# Microbiome-Based BMI Prediction Thesis

This repository contains the code for the Master's Thesis: **"Predicting BMI from Human Gut Microbiome Composition"**.

## Project Overview
This project uses machine learning (Random Forest, SVM, Decision Trees) to predict BMI from microbiome data.
The goal was to find the best model and the optimal number of samples needed.

## Results
The results plot and comparison charts are generated by the pipeline.
- **Saturation Curve:** Shows how model performance improves with more samples. This plot is found in `Nextflow/results/saturation/`.
- **Feature Importance:** Shows which bacteria matter most. Found in `Nextflow/results/`.

## Key Findings
- **Random Forest** was the best model ($R^2 = 0.387$).
- **Sample Size:** We found that 16,000 samples gave the best results.
- **Three Models Compared:** Random Forest (Best), Linear SVM (Poor), and Decision Tree (Basic).

## Repository Structure
- `Nextflow/`: Contains all the code and scripts.
- `Thesis/`: Contains the HTML reports of the code run.
- **Note:** The Data and PDF files are not included in this repository.

## How to Run
1. Install Nextflow.
2. Put data in `Data/` folder.
3. Run: `nextflow run Nextflow/main.nf -profile conda`
*   **Variance Filtering (Top 1000):** We select the **Top 1000 features** with the highest variance. This is a standard bioinformatics approach to retain features that carry the most information while reducing dimensionality.

### 2. Leakage Prevention
**Data leakage** (using testing data during training) is a critical risk in ML. We prevent this by:
*   **Separation:** Preprocessing (Scaling/Centering) is calculated *within* the workflow.
*   **Cross-Validation:** We use **5-Fold Cross-Validation** (k=5). The model is trained on 80% of the data and validated on a distinct 20% in each iteration. Hyperparameter tuning happens strictly within the training folds.

### 3. Machine Learning Models
We benchmarked three distinct algorithms to evaluate performance trade-offs:
*   **Random Forest (rf)**: Ensemble of decision trees. Robust to overfitting and handles non-linear interactions well.
*   **XGBoost (xgbTree)**: Gradient boosting framework. Highly efficient and often achieves state-of-the-art accuracy.
*   **Linear SVM (svmLinear)**: Support Vector Machine with a linear kernel. chosen for its memory efficiency (O(n)) compared to Radial kernels (O(n^2)) for this feature size.

---

## Technical Stack

*   **Workflow Engine:** [Nextflow](https://www.nextflow.io) (DSL2)
*   **Language:** R (v4.x)
*   **Libraries:** `mikropml` (pipeline), `caret` (training), `data.table` (fast I/O), `svglite` (vector graphics).
*   **Version Control:** Git & GitHub.

---

## How to Run (Usage)

### System Requirements
*   **OS:** macOS (Apple Silicon M2/M3) or Linux.
*   **RAM:** Minimum 8GB (Pipeline is optimized for sequential execution).
*   **Dependencies:** Nextflow, R, Conda.

### Execution
1.  Navigate to the workflow directory:
    ```bash
    cd Nextflow
    ```
2.  Run the pipeline:
    ```bash
    nextflow run main.nf
    ```
3.  **Outputs:**
    *   `results/model_comparison.svg`: Final publication-ready plot.
    *   `results/*.rds`: Trained model objects.

---


